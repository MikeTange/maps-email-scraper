{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPS EMAIL ADDRESS SCRAPER #\n",
    "\n",
    "_By: Michiel Tange_, _Last updated: 25/11/2024_\n",
    "\n",
    "This script scrapes email addresses off of websites of _places_ found through Google Maps in a specific area and within specific categories.\n",
    "\n",
    "It does this by first getting all relevant _places_ from Google Maps using the Places API. It finds places within a fixed radius around a point (longitude & latitude coordinates), using keywords (e.g., sports club, barber, etc.). It then gets the website for each of these _places_. Once the main website is known, it scrapes through it looking for email addresses. These email addresses are then added to an output Excel file. From here these can be used further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "from haversine import haversine\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import googlemaps\n",
    "import openpyxl\n",
    "import asyncio\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places API Key ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"APIkey.txt\", 'r') as file:\n",
    "    API_key = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places API client ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key=API_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places definitions ###\n",
    "To find places the following procedure is used:\n",
    "\n",
    "1. Draw a circle around a point (defined by longitude and latitude coordinates), of radius $r$.\n",
    "2. Use Google's Places API to find places within this circle. Places are filtered through keywords (e.g., barber, tennis club, etc.). Google will only return one page of results (20 places), so repeat $x$ amount of times using Google's 'next page token'. Save some information about each of these places - most importantly, 'place id'.\n",
    "\n",
    "NOTE: Despite only searching within the circle, Google will still return places further away than $r$. These places are additonally filtered out by computing the distance between the place and the central point of the circle using the haversine formula:\n",
    "\n",
    "$$\n",
    "a = \\sin^{2}(\\frac{\\Delta \\phi}{2}) + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\sin^{2}(\\frac{\\Delta \\lambda}{2})\n",
    "$$\n",
    "$$\n",
    "c = 2 \\cdot \\arctan 2(\\sqrt{a}, \\sqrt{1-a})\n",
    "$$\n",
    "$$\n",
    "d = \\textrm{R} \\cdot c\n",
    "$$\n",
    "Where $\\phi$ is latitude, $\\lambda$ is longitude, and $\\textrm{R}$ is the radius of Earth (approximately 6,371km).\n",
    "\n",
    "3. Use Google's Place Details API to get additional details (e.g., website) about each place, using the 'place id' as an identifier.\n",
    "4. Save all relevant information into a Pandas DataFrame. This DataFrame will form the basis for the later web scraping task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = ['website'] # details to get about a place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_check(coords : dict, second_coords : dict, dist : int) -> bool:\n",
    "    \"\"\"\n",
    "    check whether the distance between two places is smaller than a specified amount (in meters), using haversine\n",
    "\n",
    "    args\n",
    "    ----\n",
    "    coords : dict\n",
    "        dictionary containing the first set of coordinates. 'lat' for latitude, 'lng' for longitude\n",
    "    second_coords : dict\n",
    "        dictionary containing the second set of coordinates. 'lat' for latitude, 'lng' for longitude\n",
    "    dist : int\n",
    "        the distance (in meters) to check against\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    bool : bool\n",
    "        Boolean specifying whether the calculated distance is smaller than dist\n",
    "    \"\"\"\n",
    "\n",
    "    # put the first coordinates in a tuple\n",
    "    coords_tup = (coords['lat'], coords['lng'])\n",
    "    \n",
    "    # put the second coordinates in a tuple\n",
    "    second_coords_tup = (second_coords['lat'], second_coords['lng'])\n",
    "\n",
    "    # calculate the distance using haversine\n",
    "    distance = int(haversine(coords_tup, second_coords_tup, unit='m'))\n",
    "\n",
    "    return distance <= dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objects ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nearby_search_task():\n",
    "    \"\"\" a nearby search task \"\"\"\n",
    "\n",
    "    def __init__(self, coords : dict, radius : int, keywords : list, pages : int, outcomes : list=['name', 'place_id', 'types']) -> None:\n",
    "        \"\"\"\n",
    "        initialise an instance of nearby_search_task\n",
    "\n",
    "        properties\n",
    "        ----------\n",
    "        coords : dict\n",
    "            the central coordinates to search around. Provided in a dictionary with 'lat' and 'lng' as keys denoting latitude and longitude\n",
    "        radius : int\n",
    "            the search radius around the central coordinates in meters\n",
    "        keywords : list\n",
    "            the keywords to search the area for\n",
    "        pages : int\n",
    "            the maximum number of results pages to consider (20 results per page)\n",
    "        outcomes : list\n",
    "            list containing which outcome results are relevant\n",
    "        \"\"\"\n",
    "\n",
    "        self.coords = coords\n",
    "        self.radius = radius\n",
    "        self.keywords = keywords\n",
    "        self.pages = pages\n",
    "        self.outcomes = outcomes\n",
    "\n",
    "    def get_page_places(self, keyword : str, page_token : str = None) -> list:\n",
    "        \"\"\"\n",
    "        get place info through a Google Places Nearby search (limited to 20 per page)\n",
    "\n",
    "        args\n",
    "        ----\n",
    "        keyword : str\n",
    "            the keyword to search the area for\n",
    "        page_token : str\n",
    "            the page token for the search (only relevant for accessing multiple pages of the same search)\n",
    "\n",
    "        returns\n",
    "        -------\n",
    "        result : list\n",
    "            list of dictionaries where each dictionary contains a place's outcomes\n",
    "        \"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # API call\n",
    "        response = gmaps.places_nearby(location=self.coords, radius=self.radius, keyword=keyword, page_token=page_token)\n",
    "\n",
    "        # save the next page token\n",
    "        if 'next_page_token' in response:\n",
    "            results['next_page_token'] = response['next_page_token']\n",
    "        else:\n",
    "            results['next_page_token'] = 'no more pages'\n",
    "\n",
    "        # process the outcomes\n",
    "        result = []\n",
    "        for i in range(len(response['results'])):\n",
    "            \n",
    "            if distance_check(self.coords, response['results'][i]['geometry']['location'], dist=self.radius): # only include places within the radius\n",
    "                response_outcomes = {}\n",
    "                for outcome in self.outcomes:\n",
    "                    response_outcomes[outcome] = response['results'][i][outcome]\n",
    "                result.append(response_outcomes)\n",
    "\n",
    "        # save the outcomes\n",
    "        results['result'] = result\n",
    "\n",
    "        # META DATA - start\n",
    "        global places_cnt\n",
    "        places_cnt += len(response['results'])\n",
    "        # META DATA - end\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def get_all_page_places(self) -> list:\n",
    "        \"\"\"\n",
    "        get place info for all pages and all keywords through a Google Places Nearby search\n",
    "\n",
    "        returns\n",
    "        -------\n",
    "        result : list\n",
    "            list of dictionaries where each dictionary contains a place's outcomes & keyword used to find the place\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "\n",
    "        # run through all keywords and up to the maximum number of pages\n",
    "        for keyword in self.keywords:\n",
    "            page_token = None\n",
    "            for i in range(self.pages):\n",
    "                response = self.get_page_places(keyword=keyword, page_token=page_token)\n",
    "                result.append(response['result'])\n",
    "                page_token = response['next_page_token']\n",
    "                \n",
    "                if page_token == 'no more pages':\n",
    "                    break\n",
    "                \n",
    "                # timeout needed to allow Google to validate the next page token\n",
    "                time.sleep(2)\n",
    "\n",
    "        # flatten the result list (which is now a list of lists)\n",
    "        result_flat = [x for xs in result for x in xs]\n",
    "        \n",
    "        return result_flat\n",
    "\n",
    "class place_detail_search_task():\n",
    "    \"\"\" a place detail search task \"\"\"\n",
    "\n",
    "    def __init__(self, place_id : str, details : list = details) -> None:\n",
    "        \"\"\"\n",
    "        initialise an instance of place_detail_search_task\n",
    "\n",
    "        properties\n",
    "        ----------\n",
    "        place_id : str\n",
    "            the place id for which to find details\n",
    "        details : list\n",
    "            list of details to retrieve about the place\n",
    "        \"\"\"\n",
    "\n",
    "        self.place_id = place_id\n",
    "        self.details = details\n",
    "        \n",
    "    def get_place_details(self) -> dict:\n",
    "        \"\"\"\n",
    "        get details for a place\n",
    "\n",
    "        returns\n",
    "        -------\n",
    "        result : dict\n",
    "            result dictionary containing place details for a place id\n",
    "        \"\"\"\n",
    "\n",
    "        outcome = gmaps.place(self.place_id, fields=self.details)\n",
    "\n",
    "        # META DATA - start\n",
    "        if 'website' in outcome['result'].keys():\n",
    "            global websites_cnt\n",
    "            websites_cnt += 1\n",
    "        # META DATA - end\n",
    "        \n",
    "        if set(self.details).issubset(set(outcome['result'].keys())): # check if all details were returned\n",
    "            return outcome['result']\n",
    "        else: # if not all details are returned -> add detail as a key with value None - this facilitates converting to a DataFrame later\n",
    "            for detail in self.details:\n",
    "                if detail not in outcome['result'].keys():\n",
    "                    outcome['result'][detail] = None\n",
    "            return outcome['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping definitions ###\n",
    "For the webscraping an asyncronous I/O (asyncio) method is used. This allows for asyncronous server requests, and greatly improves the speed performance of the scraping. It runs through the following procedure:\n",
    "\n",
    "1. Check the homepage for an email address. If no email found -> go to step 2.\n",
    "2. Get all page links from the homepage.\n",
    "3. Check if any priority pages (common contact detail pages) are among the links found. If not -> go to step 5\n",
    "4. Check the priority pages for email addresses.\n",
    "5. Use the page links to check all other pages for email addresses.\n",
    "\n",
    "If at any point an email address is found, the process stops. Regular expressions are used to find email addresses and page links among the HTML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HREF_RE = re.compile(r'href=\"(.*?)\"') # the regular expression for finding page links\n",
    "email_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b') # the regular expression for finding email addresses\n",
    "priority_links = ['contact', 'about-us', 'over-ons', 'informatie'] # common contact detail pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objects ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parse_outcome():\n",
    "    \"\"\" the outcome of a parse_html call \"\"\"\n",
    "\n",
    "    def __init__(self, success : bool, result : set | None, html : str | None) -> None:\n",
    "        \"\"\"\n",
    "        initialise an instance of parse_outcome\n",
    "\n",
    "        properties\n",
    "        ----------\n",
    "        success : bool\n",
    "            attribute capturing whether the html parse was a success or not\n",
    "        result : set | None\n",
    "            the set of found items in the html if it was successful. None is nothing was found\n",
    "        html : str | None\n",
    "            the html which was parsed. None if the parse was a success. To be passed on after a failed parse\n",
    "        \"\"\"\n",
    "\n",
    "        self.success = success\n",
    "        self.result = result\n",
    "        self.html = html\n",
    "\n",
    "class get_outcome():\n",
    "    \"\"\" the outcome of a get_html call \"\"\"\n",
    "\n",
    "    def __init__(self, success : bool, result : str | None, descr : str) -> None:\n",
    "        \"\"\"\n",
    "        initialise an instance of get_outcome\n",
    "\n",
    "        properties\n",
    "        ----------\n",
    "        success : bool\n",
    "            attribute capturing whether the get_html was a success or not\n",
    "        result: str | None\n",
    "            the html text from a successful get. None if the get failed\n",
    "        descr : str\n",
    "            description of what happened with the call. 'Success' for a successful get, explanation if get failed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.success = success\n",
    "        self.result = result\n",
    "        self.descr = descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_html(url: str, session : ClientSession) -> get_outcome:\n",
    "    \"\"\"\n",
    "    get the html text of a url\n",
    "\n",
    "    args\n",
    "    ----\n",
    "    url : str\n",
    "        the url which to get\n",
    "    session : ClientSession\n",
    "        the session to which this call is assigned\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    get_outcome : get_outcome\n",
    "        the outcome of the get\n",
    "    \"\"\"\n",
    "\n",
    "    if 'facebook.com' not in url:\n",
    "        \n",
    "        # META DATA - start\n",
    "        global pages_cnt\n",
    "        pages_cnt += 1\n",
    "        # META DATA - end\n",
    "\n",
    "        try:\n",
    "            rsp = await session.request(method=\"GET\", url=url)\n",
    "        except: # could not connect to the url\n",
    "            return get_outcome(success=False, result=None, descr='could not connect to url')\n",
    "        else:\n",
    "            if rsp.status == 200: # success case\n",
    "                try:\n",
    "                    html = await rsp.text()\n",
    "                    return get_outcome(success=True, result=html, descr='success')\n",
    "                except: # html could not be decoded - improperly formatted on the server-side\n",
    "                    return get_outcome(success=False, result=None, descr='html could not be decoded')\n",
    "            else: # got a bad response from the url\n",
    "                return get_outcome(success=False, result=None, descr='bad response from url')\n",
    "    else:\n",
    "        return get_outcome(success=False, result=None, descr='website is a facebook page')\n",
    "\n",
    "async def parse_html(session : ClientSession, pattern : str, given_html : str | None = None, url: str | None = None) -> parse_outcome:\n",
    "    \"\"\"\n",
    "    parse a html text, looking for all occurences of a pattern\n",
    "\n",
    "    args\n",
    "    ----\n",
    "    session : ClientSession\n",
    "        the session to which this call is assigned \n",
    "    pattern : str\n",
    "        the pattern to look for in the html text\n",
    "    given_html : str | None\n",
    "        the html text to parse. Standard None -> parse_html will standard get html\n",
    "    url : str | None\n",
    "        the url for which to get and parse the html. None if html is provided via 'given_html'\n",
    "    \"\"\"\n",
    "    \n",
    "    if url != None:\n",
    "        html = await get_html(url=url, session=session)\n",
    "        html = html.result\n",
    "    else:\n",
    "        html = given_html\n",
    "\n",
    "    if html != None:\n",
    "\n",
    "        # META DATA - start\n",
    "        global words_cnt\n",
    "        words_cnt += len(html)\n",
    "        # META DATA - end\n",
    "\n",
    "        found = set(re.findall(pattern, html))\n",
    "\n",
    "        # META DATA - start\n",
    "        if pattern == email_RE:\n",
    "            global emails_cnt\n",
    "            emails_cnt += len(found)\n",
    "        # META DATA - end\n",
    "\n",
    "        if len(found) != 0: # success case\n",
    "            return parse_outcome(success=True, result=found, html=None)\n",
    "        else: # fail case\n",
    "            return parse_outcome(success=False, result=None, html=html)\n",
    "    else: # no html to parse\n",
    "        return parse_outcome(success=False, result=None, html=None)\n",
    "\n",
    "async def find_emails(starting_url : str, session : ClientSession, place_id : str, priority_links : list = priority_links) -> set | None:\n",
    "    \"\"\"\n",
    "    find emails within a website. Start on the homepage, then check priority pages, and then check all other pages.\n",
    "\n",
    "    args\n",
    "    ----\n",
    "    starting_url : str\n",
    "        the 'homepage' url from which to start, and which to search through first\n",
    "    session : ClientSession\n",
    "        the session to which this call is assigned\n",
    "    priority_links : list\n",
    "        strings commonly found in contact page names\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    result : set | None\n",
    "        set of emails found on the website. None if nothing was found\n",
    "    \"\"\"\n",
    "    \n",
    "    homepage = await parse_html(url=starting_url, session=session, pattern=email_RE)\n",
    "\n",
    "    if homepage.success:\n",
    "        return {'place_id' : place_id, 'email': homepage.result, 'url' : starting_url, 'method' : 'homepage'}\n",
    "    else:\n",
    "        # find all links on the homepage\n",
    "        links = await parse_html(given_html=homepage.html, session=session, pattern=HREF_RE)\n",
    "        if links.success:\n",
    "\n",
    "            # first check priority links\n",
    "            priority_pages_links = {urllib.parse.urljoin(starting_url, link) for link in links.result for priority_link in priority_links if priority_link in link}\n",
    "            for link in priority_pages_links:\n",
    "                link_parse = await parse_html(session=session, pattern=email_RE, url=link)\n",
    "                if link_parse.success:\n",
    "                    return {'place_id' : place_id, 'email': link_parse.result, 'url' : link, 'method': 'priority page'}\n",
    "\n",
    "            # if no success on priority pages -> check all pages\n",
    "            starting_url_stripped = urllib.parse.urlsplit(starting_url)[1]\n",
    "            url_set = set()\n",
    "            emails_found = set()\n",
    "            for link in links.result:\n",
    "                link_url = urllib.parse.urljoin(starting_url, link)\n",
    "                if ((link_url not in priority_pages_links) and (starting_url_stripped in link_url)): # skip priority pages (already checked) and links leading off the main site (usually ads)\n",
    "                    link_parse = await parse_html(session=session, pattern=email_RE, url=link_url)\n",
    "                    if link_parse.success:\n",
    "                        emails_found.update(link_parse.result)\n",
    "                        url_set.add(link_url)\n",
    "            \n",
    "            if len(emails_found) != 0:\n",
    "                return {'place_id' : place_id, 'email': emails_found, 'url' : url_set, 'method': 'regular page'}\n",
    "                    \n",
    "            # no emails found on any page\n",
    "            else:\n",
    "                return {'place_id' : place_id, 'email': None, 'url' : starting_url, 'method' : 'found nothing'}\n",
    "                \n",
    "        else: # no email or links found on homepage\n",
    "            return {'place_id' : place_id, 'email': None, 'url' : starting_url, 'method' : 'no links'}\n",
    "\n",
    "async def main(data : pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    the main function of the asyncio approach. This function gathers all the Futures (tasks that need to be executed asynchronously)\n",
    "\n",
    "    args\n",
    "    ----\n",
    "    urls : list\n",
    "        list of urls to go through\n",
    "    result : list\n",
    "        list of dictionaries containing the email addresses found for each url, and the place_id it belongs to\n",
    "    \"\"\"\n",
    "    \n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for i in range(len(data)):\n",
    "            tasks.append(find_emails(starting_url=data['website'][i], place_id=data['place_id'][i], session=session))\n",
    "        \n",
    "        result = await asyncio.gather(*tasks)\n",
    "    await session.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data ####\n",
    "Gathering meta data about the activities of the script. Denoted with  \"# META DATA\" throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_cnt = 0 # count for the number of keywords used\n",
    "places_cnt = 0 # count for the number of places investigated\n",
    "websites_cnt = 0 # count for the number of websites (\"home\" URLs) searched through\n",
    "pages_cnt = 0 # count for the number of webpages (URLs) contacted\n",
    "words_cnt = 0 # count for the number of words (HTML content) shifted through\n",
    "emails_cnt = 0 # count for the number of emails found\n",
    "useful_emails_cnt = 0 # count for the number of useful emails found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Places ####\n",
    "Reading in the search details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Search task details.txt\", 'r') as file:\n",
    "    # central point\n",
    "    line = file.readline()\n",
    "    central_point = line[line.find(':') + 2: -1]\n",
    "    # radius\n",
    "    line = file.readline()\n",
    "    radius = int(line[line.find(':') + 2: -1])\n",
    "    # keywords\n",
    "    line = file.readline()\n",
    "    keywords = line[line.find(':') + 2: ].split(', ')\n",
    "file.close()\n",
    "\n",
    "# META DATA\n",
    "keywords_cnt += len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the search task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_task_SK = nearby_search_task(coords=gmaps.geocode(central_point)[0]['geometry']['location'], # use Google's Geocoding API to find the coordinates for the central point\n",
    "                                    radius= radius,\n",
    "                                    keywords= keywords,\n",
    "                                    pages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places ##\n",
    "Finding places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the places ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_outcome = search_task_SK.get_all_page_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding place details ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in search_outcome:\n",
    "    detail_task = place_detail_search_task(place['place_id']).get_place_details() # additional details can be added here\n",
    "    if detail_task != None:\n",
    "        place['details'] = detail_task\n",
    "    else:\n",
    "        place['details'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the search outcomes into a dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(search_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'details' column is still a dictionary (it was nested before, and Pandas cannot automatically handle this). This must be converted separately, and then joined with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.DataFrame(data['details'].to_list())\n",
    "data = data.join(details_df).drop(columns=['details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates due to overlapping keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['place_id'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the actual scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_data = await main(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the new data to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, pd.DataFrame(scrape_data), on='place_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & saving the data ##\n",
    "\n",
    "NOTE: run any tweaks, tests, or bug fixes to the main script before this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing common email scraping mishaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for email_set in data['email']:\n",
    "    if email_set != None:\n",
    "        remove_set = set()\n",
    "        for email in email_set:\n",
    "            # removing common mishaps\n",
    "            if ((email.endswith('.jpg')) or (email.endswith('.png')) or (email.endswith('.gif')) or (email.endswith('.svg')) or (not ((email[-4] == '.') or (email[-3] == '.'))) or ('www.' in email) or ('.wixpress.com' in email) or ('sentry.io' in email) or ('mijnwebsite' in email) or ('@domein' in email) or ('@example' in email) or ('jouwweb' in email)): # remove false emails\n",
    "                remove_set.add(email)\n",
    "            \n",
    "            # removing duplicates due to case differences\n",
    "            if email != email.lower():\n",
    "                if email.lower() in email_set:\n",
    "                    remove_set.add(email)\n",
    "\n",
    "        if len(remove_set) != 0:\n",
    "            email_set -= remove_set\n",
    "\n",
    "        # META DATA - start\n",
    "        useful_emails_cnt += len(email_set)\n",
    "        # META DATA - end\n",
    "\n",
    "# replace empty sets created due to removing the entire email_set\n",
    "data.loc[data.email == set(), 'email'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing multiple emails for a place to spill over into additional columns, for easier use of the output Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_dict = {}\n",
    "i = 0\n",
    "for email_set in data['email']:\n",
    "    if email_set != None:\n",
    "        email_dict = {}\n",
    "        j = 1\n",
    "        for email in email_set:\n",
    "            if j <= 15: # email cap - can be changed\n",
    "                email_dict[f'email {j}'] = email\n",
    "                j += 1\n",
    "            else: # more than 15 email addresses seems excessive (found 556 for a Primera store at one point...)\n",
    "                break\n",
    "        emails_dict[data['place_id'][i]] = email_dict\n",
    "    else:\n",
    "        emails_dict[data['place_id'][i]] = {'email 1' : None}\n",
    "    i += 1\n",
    "\n",
    "# Converting to a dataframe\n",
    "emails_df = pd.DataFrame.from_dict(emails_dict, orient='index')\n",
    "emails_df['place_id'] = emails_df.index\n",
    "\n",
    "# Merging with the existing dataframe\n",
    "data = pd.merge(data, emails_df, on='place_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disgarding data which is not useful for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns\n",
    "data = data.drop(columns=['place_id', 'url', 'method', 'email'])\n",
    "\n",
    "# dropping rows with no website or emails\n",
    "data = data.dropna(ignore_index=True, subset=['website', 'email 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('Emails.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta data result ##\n",
    "The final counts of the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{keywords_cnt=}')\n",
    "print(f'{places_cnt=}')\n",
    "print(f'{websites_cnt=}')\n",
    "print(f'{pages_cnt=}')\n",
    "print(f'{words_cnt=}')\n",
    "print(f'{emails_cnt=}')\n",
    "print(f'{useful_emails_cnt=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maps_email_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
